{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "## Gradient descent\n",
    "\n",
    "Forest analogy: You're lost in the mountain and has to find your way down\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/c/c7/Okanogan-Wenatchee_National_Forest%2C_morning_fog_shrouds_trees_%2837171636495%29.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent\n",
    "\n",
    "* Gradient descent is an iterative optimization algorithm for finding the minimum of a function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Math analogy: You have a function, and the only thing you know is the *direction* it moves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In multiple dimensions the *direction* is in multiple dimensions. So we don't just need the derivative of the function in one dimension, we need it in multiple dimensions! \n",
    "\n",
    "We need the gradients! That's why it's called gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![](https://upload.wikimedia.org/wikipedia/commons/f/ff/Gradient_descent.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradients\n",
    "\n",
    "A function $f$ can require a number of inputs.\n",
    "So how do we find the gradient for one of the inputs? If we keep the others constant!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "$\\nabla f(a) = \\left(\\frac{\\partial f}{\\partial x_1}(a), \\ldots, \\frac{\\partial f}{\\partial x_n}(a)\\right)$\n",
    "\n",
    "In one point $a$, this gives us a list of partial derivatives, that tells us which direction each dimension is moving, in that exact point $a$.\n",
    "\n",
    "This is called a **gradient**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The gradient descent algorithm\n",
    "\n",
    "Let's revisit the science/suicide dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('science.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.plot.scatter(x=1, y=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We're still going for a *linear* relationship:\n",
    "\n",
    "$$f(x) = \\alpha x + \\beta$$\n",
    "\n",
    "And our loss function is still:\n",
    "\n",
    "$$l = {{\\sum^N_{i=1}(y_i - \\hat{y_i})^2} \\over N}$$\n",
    "\n",
    "Which is the same as:\n",
    "\n",
    "$$l = {{1 \\over N} \\sum^N_{i=1}(y_i - (\\alpha x_i + \\beta))^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now know what to optimise! We have a loss function with two parameters: $\\alpha$ and $\\beta$.\n",
    "\n",
    "\n",
    "$$l = {{1 \\over N} \\sum^N_{i=1}(y_i - (\\alpha x_i + \\beta))^2}$$\n",
    "\n",
    "So we have to find the **partial** derivative for $\\alpha$:\n",
    "\n",
    "$${\\partial l \\over \\partial \\alpha} = {{1 \\over N} \\sum^N_{i=1} -2x_i(y_i - (\\alpha x_i + \\beta))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now know what to optimise! We have a loss function with two parameters: $\\alpha$ and $\\beta$.\n",
    "\n",
    "\n",
    "$$l = {{1 \\over N} \\sum^N_{i=1}(y_i - (\\alpha x_i + \\beta))^2}$$\n",
    "\n",
    "So we have to find the **partial** derivative for $\\beta$:\n",
    "\n",
    "$${\\partial l \\over \\partial \\beta} = {{1 \\over N} \\sum^N_{i=1} -2(y_i - (\\alpha x_i + \\beta))}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To start gradient descent, let's just pretend that $\\alpha$ and $\\beta$ are set to 0.\n",
    "\n",
    "Whenever vi find a gradient, we get a **direction** at that specific point.\n",
    "* If the gradient is positive, the function grows\n",
    "* If the gradient is negative, the function declines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "... Either is actuall bad, because we want the function to stay at zero!\n",
    "So we will always move **in the opposite direction** of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now update our parameters:\n",
    "\n",
    "$$\\alpha \\leftarrow \\alpha - {\\partial l \\over \\partial \\alpha} $$\n",
    "\n",
    "$$\\beta \\leftarrow \\beta - {\\partial l \\over \\partial \\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning rates\n",
    "\n",
    "Gradient descent has a problem: local minima.\n",
    "\n",
    "We can add a *learning rate* that scales the amount we are learning (between 0 and 1).\n",
    "\n",
    "The learning rate will prevent us from 'jumping' too far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now update our parameters with a learning rate $\\gamma$:\n",
    "\n",
    "$$\\alpha \\leftarrow \\alpha - \\gamma {\\partial l \\over \\partial \\alpha} $$\n",
    "\n",
    "$$\\beta \\leftarrow \\beta - \\gamma {\\partial l \\over \\partial \\beta} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def loss(spendings, suimycides, a, b):\n",
    "    N = len(spendings)\n",
    "    total_error = 0.0\n",
    "    for i in range(N):\n",
    "        total_error += (suicides[i] - (a*spendings[i] + b))**2\n",
    "    return total_error / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def update_a_and_b(spendings, suicides, a, b, gamma):\n",
    "    dr_da = 0.0\n",
    "    dr_db = 0.0\n",
    "    N = len(spendings)\n",
    "\n",
    "    for i in range(N):\n",
    "        dr_da += -2 * spendings[i] * (suicides[i] - (a * spendings[i] + b))\n",
    "        dr_db += -2 * (suicides[i] - (a * spendings[i] + b))\n",
    "\n",
    "    # update a and b\n",
    "    a = a - (dr_da/float(N)) * gamma\n",
    "    b = b - (dr_db/float(N)) * gamma\n",
    "\n",
    "    return a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def train(spendings, suicides, a, b, gamma, epochs):\n",
    "    image_counter = 2\n",
    "    for e in range(epochs):\n",
    "        a, b = update_a_and_b(spendings, suicides, a, b, gamma)\n",
    "\n",
    "        if (e % 10 == 0):\n",
    "            # log the progress\n",
    "            print(\"epoch: \", str(e), \"loss: \"+str(loss(spendings, suicides, a, b)))\n",
    "            print(\"a, b: \", a, b)\n",
    "            plt.figure(image_counter)\n",
    "            axes = plt.gca()\n",
    "            plt.scatter(spendings, suicides)\n",
    "            plt.plot(spendings, spendings*a + b)\n",
    "            image_counter += 1\n",
    "            \n",
    "        if np.isnan(a):\n",
    "            raise ValueError('Infinite error!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train(df['US science spending'], df['Suicides'], 0, 0, 0.1, 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scaling data\n",
    "\n",
    "Gradient descent is sensitive to big numbers:\n",
    "\n",
    "* Large gradients = large movements = large losses = large gradients = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Solution: scale the data so it centers around 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "data = np.arange(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "StandardScaler().fit(data).transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "StandardScaler().fit(data.reshape(-1, 1)).transform(data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "StandardScaler().fit_transform(data.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "scaled_spending = StandardScaler().fit_transform(df['US science spending'].values.reshape(-1, 1))\n",
    "scaled_suicides = StandardScaler().fit_transform(df['Suicides'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train(scaled_spending, scaled_suicides, 0, 0, 0.01, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent in `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent: summary\n",
    "\n",
    "* We have a loss function for our models\n",
    "  * We like our loss to be **small**\n",
    "* That loss function can be in multiple dimensions\n",
    "  * It's **hard** to predict where the loss function is small\n",
    "* Gradients gives us an idea on the *direction* the function is going\n",
    "  * Direction **small** is good because it means a small loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gradient descent steps in the direction of the loss\n",
    "  * Until you find the smallest point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "SGDRegressor?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Gradient descent example in sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"science.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "xs = np.array(data['US science spending']).reshape(-1, 1)\n",
    "ys = np.array(data['Suicides']).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.plot.scatter(x = 1, y = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = SGDRegressor()\n",
    "model.fit(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model.predict([[100], [10000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "data.plot.scatter(x = 1, y = 2)\n",
    "plt.plot(xs, model.predict(xs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Scaling data\n",
    "\n",
    "Gradient descent goes in the direction of the gradient with respect to x.\n",
    "If that gradient is very large, the steps we take are laaaarge.\n",
    "\n",
    "What can we do to fix that?\n",
    "Scale the data to be smaller!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "xs_scaled = StandardScaler().fit_transform(xs)\n",
    "xs_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "model = SGDRegressor()\n",
    "model.fit(xs_scaled, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(xs_scaled, ys)\n",
    "plt.plot(xs_scaled, model.predict(xs_scaled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Why don't we get any good results?!\n",
    "\n",
    "To get the optimal solution, we have to take many steps towards the correct solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Exercise: \n",
    "\n",
    "* Use the `SGDRegressor` to fit a model\n",
    "* Figure out how many steps the model is taking\n",
    "* Increase that step to, say, 1000 and see whether your prediction is better\n",
    "* What happens if you increase the steps to 10000?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Recap\n",
    "\n",
    "* Derivatives\n",
    "  * The **rate of growth** for a function $f$\n",
    "  * Normally defined at points like $x$: $f'(x)$\n",
    "* Partial derivatives\n",
    "  * Derivatives in multiple dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Gradient\n",
    "  * A vector of derivatives, one for each dimension in a function $f$\n",
    "* Gradient descent\n",
    "  * A way to optimise a function by moving towards an optimum\n",
    "  * For instance minimising a loss function"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
